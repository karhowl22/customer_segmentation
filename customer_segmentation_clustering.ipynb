{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Advanced Customer Segmentation using Unsupervised Machine Learning\n",
        "## Part 2: Multiple Clustering Algorithms Implementation\n",
        "\n",
        "**Objective**: Implement and compare multiple advanced clustering algorithms to identify optimal customer segments.\n",
        "\n",
        "**Author**: [Your Name]  \n",
        "**Course**: BMCS2003 Artificial Intelligence  \n",
        "**Assignment**: Machine Learning (Unsupervised)\n",
        "\n",
        "**Excellence Features:**\n",
        "- ‚úÖ Multiple clustering algorithms (6+ methods)\n",
        "- ‚úÖ Advanced feature engineering with RFM analysis\n",
        "- ‚úÖ Comprehensive evaluation metrics\n",
        "- ‚úÖ Ensemble clustering approach\n",
        "- ‚úÖ Scalable implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Advanced clustering setup and imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Clustering algorithms\n",
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, SpectralClustering\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
        "\n",
        "# Preprocessing and evaluation\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import umap\n",
        "\n",
        "# Evaluation metrics\n",
        "from sklearn.metrics import silhouette_score, adjusted_rand_score, calinski_harabasz_score\n",
        "from sklearn.metrics import davies_bouldin_score, adjusted_mutual_info_score\n",
        "from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer\n",
        "from kneed import KneeLocator\n",
        "\n",
        "# Utilities\n",
        "import time\n",
        "from collections import Counter\n",
        "import joblib\n",
        "\n",
        "print(\"üöÄ Advanced clustering libraries imported successfully!\")\n",
        "print(\"üìä Ready for multi-algorithm customer segmentation analysis!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Loading and Advanced Feature Engineering\n",
        "Load preprocessed data and create sophisticated customer features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset (upload in Colab)\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Load and prepare data\n",
        "df = pd.read_csv('shopping_trends.csv')\n",
        "print(f\"üìä Dataset loaded: {df.shape[0]:,} customers, {df.shape[1]} features\")\n",
        "\n",
        "# Advanced RFM Feature Engineering Class\n",
        "class AdvancedCustomerFeatureEngineer:\n",
        "    \"\"\"\n",
        "    Advanced feature engineering for customer segmentation\n",
        "    Implements RFM analysis and behavioral scoring\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.frequency_mapping = {\n",
        "            'Weekly': 52, 'Bi-Weekly': 26, 'Fortnightly': 26, \n",
        "            'Monthly': 12, 'Quarterly': 4, 'Annually': 1\n",
        "        }\n",
        "        self.size_mapping = {'XS': 1, 'S': 2, 'M': 3, 'L': 4, 'XL': 5}\n",
        "    \n",
        "    def create_rfm_features(self, df):\n",
        "        \"\"\"Create RFM (Recency, Frequency, Monetary) features\"\"\"\n",
        "        df_rfm = df.copy()\n",
        "        \n",
        "        # R - Recency Score (using Review Rating as proxy for recent engagement)\n",
        "        df_rfm['Recency_Score'] = df_rfm['Review Rating']\n",
        "        \n",
        "        # F - Frequency Score\n",
        "        df_rfm['Annual_Frequency'] = df_rfm['Frequency of Purchases'].map(self.frequency_mapping)\n",
        "        df_rfm['Total_Purchase_Frequency'] = df_rfm['Previous Purchases'] * df_rfm['Annual_Frequency']\n",
        "        \n",
        "        # M - Monetary Score\n",
        "        df_rfm['Monetary_Score'] = df_rfm['Purchase Amount (USD)']\n",
        "        df_rfm['Average_Purchase_Value'] = df_rfm['Purchase Amount (USD)'] / (df_rfm['Previous Purchases'] + 1)\n",
        "        \n",
        "        # Customer Lifetime Value Proxy\n",
        "        df_rfm['CLV_Proxy'] = (df_rfm['Purchase Amount (USD)'] * \n",
        "                              df_rfm['Previous Purchases'] * \n",
        "                              df_rfm['Annual_Frequency']) / 100  # Scale down\n",
        "        \n",
        "        return df_rfm\n",
        "    \n",
        "    def create_behavioral_features(self, df):\n",
        "        \"\"\"Create advanced behavioral features\"\"\"\n",
        "        df_behavior = df.copy()\n",
        "        \n",
        "        # Binary behavioral indicators\n",
        "        df_behavior['Is_Subscribed'] = (df_behavior['Subscription Status'] == 'Yes').astype(int)\n",
        "        df_behavior['Uses_Discounts'] = (df_behavior['Discount Applied'] == 'Yes').astype(int)\n",
        "        df_behavior['Uses_Promos'] = (df_behavior['Promo Code Used'] == 'Yes').astype(int)\n",
        "        \n",
        "        # Purchase behavior score\n",
        "        df_behavior['Purchase_Behavior_Score'] = (\n",
        "            df_behavior['Is_Subscribed'] * 3 +\n",
        "            df_behavior['Uses_Discounts'] * 2 +\n",
        "            df_behavior['Uses_Promos'] * 1\n",
        "        )\n",
        "        \n",
        "        # Customer engagement level\n",
        "        conditions = [\n",
        "            df_behavior['Review Rating'] >= 4.5,\n",
        "            df_behavior['Review Rating'] >= 3.5,\n",
        "            df_behavior['Review Rating'] >= 2.5\n",
        "        ]\n",
        "        choices = [3, 2, 1]  # High, Medium, Low engagement\n",
        "        df_behavior['Engagement_Level'] = np.select(conditions, choices, default=0)\n",
        "        \n",
        "        return df_behavior\n",
        "    \n",
        "    def create_demographic_features(self, df):\n",
        "        \"\"\"Create demographic and preference features\"\"\"\n",
        "        df_demo = df.copy()\n",
        "        \n",
        "        # Age groups\n",
        "        df_demo['Age_Group'] = pd.cut(df_demo['Age'], \n",
        "                                     bins=[0, 25, 35, 50, 65, 100], \n",
        "                                     labels=['Gen_Z', 'Millennial', 'Gen_X', 'Boomer', 'Silent'])\n",
        "        \n",
        "        # Size preference (numerical)\n",
        "        df_demo['Size_Numeric'] = df_demo['Size'].map(self.size_mapping).fillna(3)\n",
        "        \n",
        "        # Gender encoding\n",
        "        df_demo['Gender_Numeric'] = (df_demo['Gender'] == 'Male').astype(int)\n",
        "        \n",
        "        # Category preferences (one-hot encoding)\n",
        "        category_dummies = pd.get_dummies(df_demo['Category'], prefix='Cat')\n",
        "        df_demo = pd.concat([df_demo, category_dummies], axis=1)\n",
        "        \n",
        "        # Season preferences\n",
        "        season_dummies = pd.get_dummies(df_demo['Season'], prefix='Season')\n",
        "        df_demo = pd.concat([df_demo, season_dummies], axis=1)\n",
        "        \n",
        "        return df_demo\n",
        "    \n",
        "    def engineer_all_features(self, df):\n",
        "        \"\"\"Complete feature engineering pipeline\"\"\"\n",
        "        print(\"üîß Starting advanced feature engineering...\")\n",
        "        \n",
        "        # Apply all transformations\n",
        "        df_engineered = self.create_rfm_features(df)\n",
        "        df_engineered = self.create_behavioral_features(df_engineered)\n",
        "        df_engineered = self.create_demographic_features(df_engineered)\n",
        "        \n",
        "        print(f\"‚úÖ Feature engineering completed! Shape: {df_engineered.shape}\")\n",
        "        return df_engineered\n",
        "\n",
        "# Apply feature engineering\n",
        "feature_engineer = AdvancedCustomerFeatureEngineer()\n",
        "df_engineered = feature_engineer.engineer_all_features(df)\n",
        "\n",
        "print(f\"\\nüìà Final dataset shape: {df_engineered.shape}\")\n",
        "print(f\"üéØ Ready for clustering analysis!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Advanced Clustering Preparation\n",
        "Feature selection and scaling for optimal clustering performance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Advanced Feature Selection for Clustering\n",
        "class ClusteringPreprocessor:\n",
        "    \"\"\"\n",
        "    Advanced preprocessing pipeline for clustering analysis\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.scalers = {\n",
        "            'standard': StandardScaler(),\n",
        "            'minmax': MinMaxScaler(),\n",
        "            'robust': RobustScaler()\n",
        "        }\n",
        "        self.selected_features = None\n",
        "        self.scaler = None\n",
        "    \n",
        "    def select_clustering_features(self, df_engineered):\n",
        "        \"\"\"Select optimal features for clustering\"\"\"\n",
        "        \n",
        "        # Core RFM features\n",
        "        rfm_features = [\n",
        "            'Recency_Score', 'Total_Purchase_Frequency', 'Monetary_Score',\n",
        "            'Average_Purchase_Value', 'CLV_Proxy'\n",
        "        ]\n",
        "        \n",
        "        # Behavioral features\n",
        "        behavioral_features = [\n",
        "            'Is_Subscribed', 'Uses_Discounts', 'Uses_Promos',\n",
        "            'Purchase_Behavior_Score', 'Engagement_Level'\n",
        "        ]\n",
        "        \n",
        "        # Demographic features\n",
        "        demographic_features = [\n",
        "            'Age', 'Gender_Numeric', 'Size_Numeric'\n",
        "        ]\n",
        "        \n",
        "        # Category preferences\n",
        "        category_features = [col for col in df_engineered.columns if col.startswith('Cat_')]\n",
        "        \n",
        "        # Season preferences\n",
        "        season_features = [col for col in df_engineered.columns if col.startswith('Season_')]\n",
        "        \n",
        "        # Additional purchase features\n",
        "        purchase_features = ['Previous Purchases', 'Annual_Frequency']\n",
        "        \n",
        "        # Combine all features\n",
        "        self.selected_features = (rfm_features + behavioral_features + \n",
        "                                demographic_features + category_features + \n",
        "                                season_features + purchase_features)\n",
        "        \n",
        "        # Ensure all features exist in dataframe\n",
        "        existing_features = [f for f in self.selected_features if f in df_engineered.columns]\n",
        "        \n",
        "        print(f\"üéØ Selected {len(existing_features)} features for clustering:\")\n",
        "        print(\"üìä Feature Categories:\")\n",
        "        print(f\"  - RFM Features: {len(rfm_features)}\")\n",
        "        print(f\"  - Behavioral Features: {len(behavioral_features)}\")\n",
        "        print(f\"  - Demographic Features: {len(demographic_features)}\")\n",
        "        print(f\"  - Category Features: {len(category_features)}\")\n",
        "        print(f\"  - Season Features: {len(season_features)}\")\n",
        "        print(f\"  - Purchase Features: {len(purchase_features)}\")\n",
        "        \n",
        "        return df_engineered[existing_features]\n",
        "    \n",
        "    def prepare_data(self, df_engineered, scaler_type='standard'):\n",
        "        \"\"\"Complete data preparation pipeline\"\"\"\n",
        "        \n",
        "        # Select features\n",
        "        X_features = self.select_clustering_features(df_engineered)\n",
        "        \n",
        "        # Handle missing values\n",
        "        X_features = X_features.fillna(X_features.median())\n",
        "        \n",
        "        # Scale features\n",
        "        self.scaler = self.scalers[scaler_type]\n",
        "        X_scaled = self.scaler.fit_transform(X_features)\n",
        "        X_scaled = pd.DataFrame(X_scaled, columns=X_features.columns)\n",
        "        \n",
        "        print(f\"‚úÖ Data preparation completed!\")\n",
        "        print(f\"üìä Final clustering dataset shape: {X_scaled.shape}\")\n",
        "        print(f\"üîß Scaling method: {scaler_type}\")\n",
        "        \n",
        "        return X_scaled, X_features\n",
        "\n",
        "# Initialize preprocessor\n",
        "preprocessor = ClusteringPreprocessor()\n",
        "X_scaled, X_features = preprocessor.prepare_data(df_engineered, scaler_type='standard')\n",
        "\n",
        "# Display feature summary\n",
        "print(\"\\nüìà CLUSTERING FEATURES SUMMARY:\")\n",
        "print(\"=\"*50)\n",
        "display(X_scaled.describe().round(3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Multiple Clustering Algorithms Implementation\n",
        "Advanced implementation of 6+ clustering algorithms for comprehensive analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Advanced Multi-Algorithm Clustering Framework\n",
        "class AdvancedClusteringFramework:\n",
        "    \"\"\"\n",
        "    Comprehensive clustering framework implementing multiple algorithms\n",
        "    for customer segmentation analysis\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, X_scaled):\n",
        "        self.X = X_scaled\n",
        "        self.results = {}\n",
        "        self.evaluation_metrics = {}\n",
        "        self.optimal_clusters = {}\n",
        "        \n",
        "    def find_optimal_clusters_kmeans(self, max_k=15):\n",
        "        \"\"\"Find optimal number of clusters using multiple methods\"\"\"\n",
        "        print(\"üîç Finding optimal number of clusters for K-Means...\")\n",
        "        \n",
        "        # Elbow Method\n",
        "        inertias = []\n",
        "        silhouette_scores = []\n",
        "        k_range = range(2, max_k + 1)\n",
        "        \n",
        "        for k in k_range:\n",
        "            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "            labels = kmeans.fit_predict(self.X)\n",
        "            inertias.append(kmeans.inertia_)\n",
        "            silhouette_scores.append(silhouette_score(self.X, labels))\n",
        "        \n",
        "        # Find elbow using KneeLocator\n",
        "        knee_locator = KneeLocator(k_range, inertias, curve='convex', direction='decreasing')\n",
        "        elbow_k = knee_locator.elbow if knee_locator.elbow else 4\n",
        "        \n",
        "        # Best silhouette score\n",
        "        best_silhouette_k = k_range[np.argmax(silhouette_scores)]\n",
        "        \n",
        "        print(f\"üìä Elbow method suggests: {elbow_k} clusters\")\n",
        "        print(f\"üìà Best silhouette score at: {best_silhouette_k} clusters\")\n",
        "        print(f\"üéØ Selected optimal clusters: {elbow_k}\")\n",
        "        \n",
        "        return elbow_k, inertias, silhouette_scores\n",
        "    \n",
        "    def kmeans_clustering(self, n_clusters=None):\n",
        "        \"\"\"Implement K-Means clustering\"\"\"\n",
        "        print(\"üî¨ Implementing K-Means Clustering...\")\n",
        "        \n",
        "        if n_clusters is None:\n",
        "            n_clusters, _, _ = self.find_optimal_clusters_kmeans()\n",
        "        \n",
        "        # Fit K-Means\n",
        "        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
        "        labels = kmeans.fit_predict(self.X)\n",
        "        \n",
        "        # Store results\n",
        "        self.results['kmeans'] = {\n",
        "            'labels': labels,\n",
        "            'model': kmeans,\n",
        "            'n_clusters': n_clusters,\n",
        "            'centroids': kmeans.cluster_centers_\n",
        "        }\n",
        "        \n",
        "        print(f\"‚úÖ K-Means completed with {n_clusters} clusters\")\n",
        "        return labels\n",
        "    \n",
        "    def dbscan_clustering(self, eps=None, min_samples=None):\n",
        "        \"\"\"Implement DBSCAN clustering\"\"\"\n",
        "        print(\"üî¨ Implementing DBSCAN Clustering...\")\n",
        "        \n",
        "        # Auto-tune parameters if not provided\n",
        "        if eps is None:\n",
        "            # Use average distance to k-nearest neighbors\n",
        "            from sklearn.neighbors import NearestNeighbors\n",
        "            neighbors = NearestNeighbors(n_neighbors=5)\n",
        "            neighbors_fit = neighbors.fit(self.X)\n",
        "            distances, indices = neighbors_fit.kneighbors(self.X)\n",
        "            distances = np.sort(distances, axis=0)\n",
        "            distances = distances[:, 1]\n",
        "            eps = np.percentile(distances, 90)  # Use 90th percentile\n",
        "        \n",
        "        if min_samples is None:\n",
        "            min_samples = max(5, len(self.X.columns))  # Based on dimensionality\n",
        "        \n",
        "        # Fit DBSCAN\n",
        "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "        labels = dbscan.fit_predict(self.X)\n",
        "        \n",
        "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "        n_noise = list(labels).count(-1)\n",
        "        \n",
        "        # Store results\n",
        "        self.results['dbscan'] = {\n",
        "            'labels': labels,\n",
        "            'model': dbscan,\n",
        "            'n_clusters': n_clusters,\n",
        "            'n_noise': n_noise,\n",
        "            'eps': eps,\n",
        "            'min_samples': min_samples\n",
        "        }\n",
        "        \n",
        "        print(f\"‚úÖ DBSCAN completed with {n_clusters} clusters and {n_noise} noise points\")\n",
        "        return labels\n",
        "    \n",
        "    def gaussian_mixture_clustering(self, n_clusters=None):\n",
        "        \"\"\"Implement Gaussian Mixture Model clustering\"\"\"\n",
        "        print(\"üî¨ Implementing Gaussian Mixture Model...\")\n",
        "        \n",
        "        if n_clusters is None:\n",
        "            # Use BIC to find optimal number of components\n",
        "            bic_scores = []\n",
        "            k_range = range(2, 12)\n",
        "            for k in k_range:\n",
        "                gmm = GaussianMixture(n_components=k, random_state=42)\n",
        "                gmm.fit(self.X)\n",
        "                bic_scores.append(gmm.bic(self.X))\n",
        "            \n",
        "            optimal_k = k_range[np.argmin(bic_scores)]\n",
        "            print(f\"üìä BIC suggests {optimal_k} components\")\n",
        "        else:\n",
        "            optimal_k = n_clusters\n",
        "        \n",
        "        # Fit GMM\n",
        "        gmm = GaussianMixture(n_components=optimal_k, random_state=42)\n",
        "        labels = gmm.fit_predict(self.X)\n",
        "        \n",
        "        # Store results\n",
        "        self.results['gmm'] = {\n",
        "            'labels': labels,\n",
        "            'model': gmm,\n",
        "            'n_clusters': optimal_k,\n",
        "            'probabilities': gmm.predict_proba(self.X)\n",
        "        }\n",
        "        \n",
        "        print(f\"‚úÖ GMM completed with {optimal_k} components\")\n",
        "        return labels\n",
        "    \n",
        "    def hierarchical_clustering(self, n_clusters=None, linkage='ward'):\n",
        "        \"\"\"Implement Agglomerative Hierarchical clustering\"\"\"\n",
        "        print(\"üî¨ Implementing Hierarchical Clustering...\")\n",
        "        \n",
        "        if n_clusters is None:\n",
        "            n_clusters = 5  # Default\n",
        "        \n",
        "        # Fit Hierarchical clustering\n",
        "        hierarchical = AgglomerativeClustering(n_clusters=n_clusters, linkage=linkage)\n",
        "        labels = hierarchical.fit_predict(self.X)\n",
        "        \n",
        "        # Store results\n",
        "        self.results['hierarchical'] = {\n",
        "            'labels': labels,\n",
        "            'model': hierarchical,\n",
        "            'n_clusters': n_clusters,\n",
        "            'linkage': linkage\n",
        "        }\n",
        "        \n",
        "        print(f\"‚úÖ Hierarchical clustering completed with {n_clusters} clusters\")\n",
        "        return labels\n",
        "    \n",
        "    def spectral_clustering(self, n_clusters=None):\n",
        "        \"\"\"Implement Spectral clustering\"\"\"\n",
        "        print(\"üî¨ Implementing Spectral Clustering...\")\n",
        "        \n",
        "        if n_clusters is None:\n",
        "            n_clusters = 5  # Default\n",
        "        \n",
        "        # Fit Spectral clustering\n",
        "        spectral = SpectralClustering(n_clusters=n_clusters, random_state=42, \n",
        "                                    affinity='rbf', gamma=1.0)\n",
        "        labels = spectral.fit_predict(self.X)\n",
        "        \n",
        "        # Store results\n",
        "        self.results['spectral'] = {\n",
        "            'labels': labels,\n",
        "            'model': spectral,\n",
        "            'n_clusters': n_clusters\n",
        "        }\n",
        "        \n",
        "        print(f\"‚úÖ Spectral clustering completed with {n_clusters} clusters\")\n",
        "        return labels\n",
        "    \n",
        "    def meanshift_clustering(self):\n",
        "        \"\"\"Implement Mean Shift clustering\"\"\"\n",
        "        print(\"üî¨ Implementing Mean Shift Clustering...\")\n",
        "        \n",
        "        # Estimate bandwidth\n",
        "        bandwidth = estimate_bandwidth(self.X, quantile=0.2, n_samples=500)\n",
        "        \n",
        "        # Fit Mean Shift\n",
        "        meanshift = MeanShift(bandwidth=bandwidth)\n",
        "        labels = meanshift.fit_predict(self.X)\n",
        "        \n",
        "        n_clusters = len(set(labels))\n",
        "        \n",
        "        # Store results\n",
        "        self.results['meanshift'] = {\n",
        "            'labels': labels,\n",
        "            'model': meanshift,\n",
        "            'n_clusters': n_clusters,\n",
        "            'bandwidth': bandwidth\n",
        "        }\n",
        "        \n",
        "        print(f\"‚úÖ Mean Shift completed with {n_clusters} clusters\")\n",
        "        return labels\n",
        "    \n",
        "    def run_all_algorithms(self, n_clusters_base=5):\n",
        "        \"\"\"Run all clustering algorithms\"\"\"\n",
        "        print(\"üöÄ RUNNING ALL CLUSTERING ALGORITHMS\")\n",
        "        print(\"=\"*60)\n",
        "        \n",
        "        algorithms = [\n",
        "            ('K-Means', lambda: self.kmeans_clustering()),\n",
        "            ('DBSCAN', lambda: self.dbscan_clustering()),\n",
        "            ('Gaussian Mixture', lambda: self.gaussian_mixture_clustering()),\n",
        "            ('Hierarchical', lambda: self.hierarchical_clustering(n_clusters_base)),\n",
        "            ('Spectral', lambda: self.spectral_clustering(n_clusters_base)),\n",
        "            ('Mean Shift', lambda: self.meanshift_clustering())\n",
        "        ]\n",
        "        \n",
        "        for name, algorithm in algorithms:\n",
        "            start_time = time.time()\n",
        "            try:\n",
        "                algorithm()\n",
        "                end_time = time.time()\n",
        "                print(f\"‚è±Ô∏è {name} completed in {end_time - start_time:.2f} seconds\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå {name} failed: {str(e)}\")\n",
        "            print(\"-\" * 40)\n",
        "        \n",
        "        print(\"üéâ All algorithms completed!\")\n",
        "        return self.results\n",
        "\n",
        "# Initialize and run clustering framework\n",
        "print(\"üî• ADVANCED CLUSTERING ANALYSIS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "clustering_framework = AdvancedClusteringFramework(X_scaled)\n",
        "results = clustering_framework.run_all_algorithms()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Comprehensive Clustering Evaluation\n",
        "Advanced evaluation metrics for clustering quality assessment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Advanced Clustering Evaluation Framework\n",
        "class ClusteringEvaluator:\n",
        "    \"\"\"\n",
        "    Comprehensive evaluation framework for clustering algorithms\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, X, results):\n",
        "        self.X = X\n",
        "        self.results = results\n",
        "        self.evaluation_results = {}\n",
        "    \n",
        "    def calculate_internal_metrics(self, labels, algorithm_name):\n",
        "        \"\"\"Calculate internal validation metrics\"\"\"\n",
        "        \n",
        "        # Skip if all points are noise or single cluster\n",
        "        unique_labels = set(labels)\n",
        "        if len(unique_labels) < 2 or (len(unique_labels) == 2 and -1 in unique_labels):\n",
        "            return {\n",
        "                'silhouette_score': -1,\n",
        "                'calinski_harabasz_score': 0,\n",
        "                'davies_bouldin_score': float('inf')\n",
        "            }\n",
        "        \n",
        "        # Filter out noise points for metrics that don't handle them\n",
        "        if -1 in labels:\n",
        "            mask = labels != -1\n",
        "            X_filtered = self.X[mask]\n",
        "            labels_filtered = labels[mask]\n",
        "        else:\n",
        "            X_filtered = self.X\n",
        "            labels_filtered = labels\n",
        "        \n",
        "        # Skip if too few points remain\n",
        "        if len(X_filtered) < 2 or len(set(labels_filtered)) < 2:\n",
        "            return {\n",
        "                'silhouette_score': -1,\n",
        "                'calinski_harabasz_score': 0,\n",
        "                'davies_bouldin_score': float('inf')\n",
        "            }\n",
        "        \n",
        "        try:\n",
        "            # Silhouette Score (higher is better, range: -1 to 1)\n",
        "            sil_score = silhouette_score(X_filtered, labels_filtered)\n",
        "            \n",
        "            # Calinski-Harabasz Index (higher is better)\n",
        "            ch_score = calinski_harabasz_score(X_filtered, labels_filtered)\n",
        "            \n",
        "            # Davies-Bouldin Index (lower is better)\n",
        "            db_score = davies_bouldin_score(X_filtered, labels_filtered)\n",
        "            \n",
        "            return {\n",
        "                'silhouette_score': sil_score,\n",
        "                'calinski_harabasz_score': ch_score,\n",
        "                'davies_bouldin_score': db_score\n",
        "            }\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error calculating metrics for {algorithm_name}: {str(e)}\")\n",
        "            return {\n",
        "                'silhouette_score': -1,\n",
        "                'calinski_harabasz_score': 0,\n",
        "                'davies_bouldin_score': float('inf')\n",
        "            }\n",
        "    \n",
        "    def calculate_cluster_statistics(self, labels, algorithm_name):\n",
        "        \"\"\"Calculate cluster distribution statistics\"\"\"\n",
        "        \n",
        "        unique_labels = np.unique(labels)\n",
        "        n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)\n",
        "        n_noise = np.sum(labels == -1) if -1 in labels else 0\n",
        "        \n",
        "        # Cluster sizes\n",
        "        cluster_sizes = []\n",
        "        for label in unique_labels:\n",
        "            if label != -1:  # Exclude noise\n",
        "                cluster_sizes.append(np.sum(labels == label))\n",
        "        \n",
        "        if cluster_sizes:\n",
        "            avg_cluster_size = np.mean(cluster_sizes)\n",
        "            std_cluster_size = np.std(cluster_sizes)\n",
        "            min_cluster_size = np.min(cluster_sizes)\n",
        "            max_cluster_size = np.max(cluster_sizes)\n",
        "        else:\n",
        "            avg_cluster_size = std_cluster_size = min_cluster_size = max_cluster_size = 0\n",
        "        \n",
        "        return {\n",
        "            'n_clusters': n_clusters,\n",
        "            'n_noise': n_noise,\n",
        "            'noise_ratio': n_noise / len(labels),\n",
        "            'avg_cluster_size': avg_cluster_size,\n",
        "            'std_cluster_size': std_cluster_size,\n",
        "            'min_cluster_size': min_cluster_size,\n",
        "            'max_cluster_size': max_cluster_size,\n",
        "            'cluster_balance': std_cluster_size / avg_cluster_size if avg_cluster_size > 0 else 0\n",
        "        }\n",
        "    \n",
        "    def evaluate_all_algorithms(self):\n",
        "        \"\"\"Evaluate all clustering algorithms\"\"\"\n",
        "        print(\"üìä COMPREHENSIVE CLUSTERING EVALUATION\")\n",
        "        print(\"=\"*60)\n",
        "        \n",
        "        evaluation_summary = []\n",
        "        \n",
        "        for algorithm_name, result in self.results.items():\n",
        "            print(f\"\\\\nüîç Evaluating {algorithm_name.upper()}...\")\n",
        "            \n",
        "            labels = result['labels']\n",
        "            \n",
        "            # Calculate metrics\n",
        "            internal_metrics = self.calculate_internal_metrics(labels, algorithm_name)\n",
        "            cluster_stats = self.calculate_cluster_statistics(labels, algorithm_name)\n",
        "            \n",
        "            # Combine all metrics\n",
        "            evaluation = {\n",
        "                'algorithm': algorithm_name,\n",
        "                **internal_metrics,\n",
        "                **cluster_stats\n",
        "            }\n",
        "            \n",
        "            self.evaluation_results[algorithm_name] = evaluation\n",
        "            evaluation_summary.append(evaluation)\n",
        "            \n",
        "            # Print summary\n",
        "            print(f\"  üìà Silhouette Score: {internal_metrics['silhouette_score']:.3f}\")\n",
        "            print(f\"  üìä Calinski-Harabasz: {internal_metrics['calinski_harabasz_score']:.1f}\")\n",
        "            print(f\"  üìâ Davies-Bouldin: {internal_metrics['davies_bouldin_score']:.3f}\")\n",
        "            print(f\"  üéØ Clusters: {cluster_stats['n_clusters']}\")\n",
        "            print(f\"  üîá Noise Points: {cluster_stats['n_noise']}\")\n",
        "            \n",
        "        return pd.DataFrame(evaluation_summary)\n",
        "    \n",
        "    def create_comparison_table(self, df_evaluation):\n",
        "        \"\"\"Create formatted comparison table\"\"\"\n",
        "        \n",
        "        # Select key metrics for comparison\n",
        "        comparison_cols = [\n",
        "            'algorithm', 'silhouette_score', 'calinski_harabasz_score', \n",
        "            'davies_bouldin_score', 'n_clusters', 'noise_ratio'\n",
        "        ]\n",
        "        \n",
        "        df_comparison = df_evaluation[comparison_cols].copy()\n",
        "        \n",
        "        # Round numerical values\n",
        "        df_comparison['silhouette_score'] = df_comparison['silhouette_score'].round(3)\n",
        "        df_comparison['calinski_harabasz_score'] = df_comparison['calinski_harabasz_score'].round(1)\n",
        "        df_comparison['davies_bouldin_score'] = df_comparison['davies_bouldin_score'].round(3)\n",
        "        df_comparison['noise_ratio'] = (df_comparison['noise_ratio'] * 100).round(1)\n",
        "        \n",
        "        # Rename columns for better display\n",
        "        df_comparison.columns = [\n",
        "            'Algorithm', 'Silhouette', 'Calinski-Harabasz', \n",
        "            'Davies-Bouldin', 'Clusters', 'Noise %'\n",
        "        ]\n",
        "        \n",
        "        return df_comparison\n",
        "    \n",
        "    def rank_algorithms(self, df_evaluation):\n",
        "        \"\"\"Rank algorithms based on multiple criteria\"\"\"\n",
        "        \n",
        "        # Create ranking scores (normalize metrics to 0-1 scale)\n",
        "        df_rank = df_evaluation.copy()\n",
        "        \n",
        "        # Higher is better metrics\n",
        "        df_rank['sil_rank'] = df_rank['silhouette_score'].rank(ascending=False)\n",
        "        df_rank['ch_rank'] = df_rank['calinski_harabasz_score'].rank(ascending=False)\n",
        "        \n",
        "        # Lower is better metrics\n",
        "        df_rank['db_rank'] = df_rank['davies_bouldin_score'].rank(ascending=True)\n",
        "        df_rank['noise_rank'] = df_rank['noise_ratio'].rank(ascending=True)\n",
        "        \n",
        "        # Combined ranking (equal weights)\n",
        "        df_rank['overall_rank'] = (\n",
        "            df_rank['sil_rank'] + df_rank['ch_rank'] + \n",
        "            df_rank['db_rank'] + df_rank['noise_rank']\n",
        "        ) / 4\n",
        "        \n",
        "        # Sort by overall ranking\n",
        "        df_rank = df_rank.sort_values('overall_rank')\n",
        "        \n",
        "        return df_rank[['algorithm', 'overall_rank', 'sil_rank', 'ch_rank', 'db_rank']]\n",
        "\n",
        "# Run comprehensive evaluation\n",
        "evaluator = ClusteringEvaluator(X_scaled.values, results)\n",
        "df_evaluation = evaluator.evaluate_all_algorithms()\n",
        "\n",
        "print(\"\\\\nüèÜ CLUSTERING ALGORITHM COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Display comparison table\n",
        "df_comparison = evaluator.create_comparison_table(df_evaluation)\n",
        "display(df_comparison)\n",
        "\n",
        "# Display algorithm rankings\n",
        "print(\"\\\\nü•á ALGORITHM RANKINGS\")\n",
        "print(\"=\"*40)\n",
        "df_rankings = evaluator.rank_algorithms(df_evaluation)\n",
        "display(df_rankings)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Advanced Cluster Visualization & Analysis\n",
        "Interactive visualizations and dimensionality reduction for cluster interpretation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Advanced Cluster Visualization Framework\n",
        "class AdvancedClusterVisualizer:\n",
        "    \"\"\"\n",
        "    Comprehensive visualization framework for cluster analysis\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, X_scaled, X_original, results, df_original):\n",
        "        self.X_scaled = X_scaled\n",
        "        self.X_original = X_original\n",
        "        self.results = results\n",
        "        self.df_original = df_original\n",
        "        \n",
        "    def apply_dimensionality_reduction(self):\n",
        "        \"\"\"Apply multiple dimensionality reduction techniques\"\"\"\n",
        "        print(\"üîç Applying dimensionality reduction techniques...\")\n",
        "        \n",
        "        # PCA\n",
        "        pca = PCA(n_components=2, random_state=42)\n",
        "        X_pca = pca.fit_transform(self.X_scaled)\n",
        "        \n",
        "        # t-SNE\n",
        "        tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
        "        X_tsne = tsne.fit_transform(self.X_scaled)\n",
        "        \n",
        "        # UMAP\n",
        "        reducer = umap.UMAP(n_components=2, random_state=42)\n",
        "        X_umap = reducer.fit_transform(self.X_scaled)\n",
        "        \n",
        "        print(f\"‚úÖ PCA explained variance: {pca.explained_variance_ratio_.sum():.3f}\")\n",
        "        \n",
        "        return {\n",
        "            'pca': X_pca,\n",
        "            'tsne': X_tsne,\n",
        "            'umap': X_umap,\n",
        "            'pca_model': pca\n",
        "        }\n",
        "    \n",
        "    def create_cluster_comparison_plot(self, reduction_data, selected_algorithms=None):\n",
        "        \"\"\"Create comprehensive cluster comparison visualization\"\"\"\n",
        "        \n",
        "        if selected_algorithms is None:\n",
        "            selected_algorithms = ['kmeans', 'dbscan', 'gmm', 'hierarchical']\n",
        "        \n",
        "        # Filter algorithms that exist in results\n",
        "        available_algorithms = [alg for alg in selected_algorithms if alg in self.results]\n",
        "        \n",
        "        fig = make_subplots(\n",
        "            rows=len(available_algorithms), \n",
        "            cols=3,\n",
        "            subplot_titles=[f'{method.upper()}' for method in ['PCA', 't-SNE', 'UMAP']] * len(available_algorithms),\n",
        "            vertical_spacing=0.08,\n",
        "            horizontal_spacing=0.05\n",
        "        )\n",
        "        \n",
        "        colors = px.colors.qualitative.Set3\n",
        "        \n",
        "        for i, algorithm in enumerate(available_algorithms):\n",
        "            labels = self.results[algorithm]['labels']\n",
        "            unique_labels = np.unique(labels)\n",
        "            \n",
        "            # PCA plot\n",
        "            for j, label in enumerate(unique_labels):\n",
        "                mask = labels == label\n",
        "                label_name = f'Cluster {label}' if label != -1 else 'Noise'\n",
        "                color = colors[j % len(colors)]\n",
        "                \n",
        "                fig.add_trace(\n",
        "                    go.Scatter(\n",
        "                        x=reduction_data['pca'][mask, 0],\n",
        "                        y=reduction_data['pca'][mask, 1],\n",
        "                        mode='markers',\n",
        "                        name=f'{algorithm}_{label_name}',\n",
        "                        marker=dict(color=color, size=4, opacity=0.7),\n",
        "                        showlegend=(i == 0)  # Show legend only for first algorithm\n",
        "                    ),\n",
        "                    row=i+1, col=1\n",
        "                )\n",
        "                \n",
        "                # t-SNE plot\n",
        "                fig.add_trace(\n",
        "                    go.Scatter(\n",
        "                        x=reduction_data['tsne'][mask, 0],\n",
        "                        y=reduction_data['tsne'][mask, 1],\n",
        "                        mode='markers',\n",
        "                        name=f'{algorithm}_{label_name}',\n",
        "                        marker=dict(color=color, size=4, opacity=0.7),\n",
        "                        showlegend=False\n",
        "                    ),\n",
        "                    row=i+1, col=2\n",
        "                )\n",
        "                \n",
        "                # UMAP plot\n",
        "                fig.add_trace(\n",
        "                    go.Scatter(\n",
        "                        x=reduction_data['umap'][mask, 0],\n",
        "                        y=reduction_data['umap'][mask, 1],\n",
        "                        mode='markers',\n",
        "                        name=f'{algorithm}_{label_name}',\n",
        "                        marker=dict(color=color, size=4, opacity=0.7),\n",
        "                        showlegend=False\n",
        "                    ),\n",
        "                    row=i+1, col=3\n",
        "                )\n",
        "        \n",
        "        fig.update_layout(\n",
        "            height=300 * len(available_algorithms),\n",
        "            title_text=\"Cluster Comparison Across Dimensionality Reduction Methods\",\n",
        "            title_x=0.5\n",
        "        )\n",
        "        \n",
        "        fig.show()\n",
        "    \n",
        "    def create_business_insights_dashboard(self, best_algorithm='kmeans'):\n",
        "        \"\"\"Create business-focused cluster analysis dashboard\"\"\"\n",
        "        \n",
        "        if best_algorithm not in self.results:\n",
        "            best_algorithm = list(self.results.keys())[0]\n",
        "        \n",
        "        labels = self.results[best_algorithm]['labels']\n",
        "        \n",
        "        # Add cluster labels to original dataframe\n",
        "        df_analysis = self.df_original.copy()\n",
        "        df_analysis['Cluster'] = labels\n",
        "        \n",
        "        # Remove noise points for business analysis\n",
        "        if -1 in labels:\n",
        "            df_analysis = df_analysis[df_analysis['Cluster'] != -1]\n",
        "        \n",
        "        # Create business insights dashboard\n",
        "        fig = make_subplots(\n",
        "            rows=2, cols=2,\n",
        "            subplot_titles=('Customer Value by Cluster', 'Age Distribution by Cluster',\n",
        "                           'Purchase Frequency by Cluster', 'Category Preferences by Cluster'),\n",
        "            specs=[[{'type': 'box'}, {'type': 'histogram'}],\n",
        "                   [{'type': 'bar'}, {'type': 'bar'}]]\n",
        "        )\n",
        "        \n",
        "        unique_clusters = sorted(df_analysis['Cluster'].unique())\n",
        "        colors = px.colors.qualitative.Set3\n",
        "        \n",
        "        # Customer Value Analysis\n",
        "        for i, cluster in enumerate(unique_clusters):\n",
        "            cluster_data = df_analysis[df_analysis['Cluster'] == cluster]\n",
        "            fig.add_trace(\n",
        "                go.Box(\n",
        "                    y=cluster_data['Purchase Amount (USD)'],\n",
        "                    name=f'Cluster {cluster}',\n",
        "                    marker_color=colors[i % len(colors)]\n",
        "                ),\n",
        "                row=1, col=1\n",
        "            )\n",
        "        \n",
        "        # Age Distribution\n",
        "        for i, cluster in enumerate(unique_clusters):\n",
        "            cluster_data = df_analysis[df_analysis['Cluster'] == cluster]\n",
        "            fig.add_trace(\n",
        "                go.Histogram(\n",
        "                    x=cluster_data['Age'],\n",
        "                    name=f'Cluster {cluster}',\n",
        "                    marker_color=colors[i % len(colors)],\n",
        "                    opacity=0.7,\n",
        "                    nbinsx=20\n",
        "                ),\n",
        "                row=1, col=2\n",
        "            )\n",
        "        \n",
        "        # Purchase Frequency Analysis\n",
        "        freq_by_cluster = df_analysis.groupby(['Cluster', 'Frequency of Purchases']).size().reset_index(name='Count')\n",
        "        for i, cluster in enumerate(unique_clusters):\n",
        "            cluster_freq = freq_by_cluster[freq_by_cluster['Cluster'] == cluster]\n",
        "            fig.add_trace(\n",
        "                go.Bar(\n",
        "                    x=cluster_freq['Frequency of Purchases'],\n",
        "                    y=cluster_freq['Count'],\n",
        "                    name=f'Cluster {cluster}',\n",
        "                    marker_color=colors[i % len(colors)]\n",
        "                ),\n",
        "                row=2, col=1\n",
        "            )\n",
        "        \n",
        "        # Category Preferences\n",
        "        cat_by_cluster = df_analysis.groupby(['Cluster', 'Category']).size().reset_index(name='Count')\n",
        "        for i, cluster in enumerate(unique_clusters):\n",
        "            cluster_cat = cat_by_cluster[cat_by_cluster['Cluster'] == cluster]\n",
        "            fig.add_trace(\n",
        "                go.Bar(\n",
        "                    x=cluster_cat['Category'],\n",
        "                    y=cluster_cat['Count'],\n",
        "                    name=f'Cluster {cluster}',\n",
        "                    marker_color=colors[i % len(colors)]\n",
        "                ),\n",
        "                row=2, col=2\n",
        "            )\n",
        "        \n",
        "        fig.update_layout(\n",
        "            height=800,\n",
        "            title_text=f\"Business Insights Dashboard - {best_algorithm.upper()} Clustering\",\n",
        "            title_x=0.5,\n",
        "            showlegend=True\n",
        "        )\n",
        "        \n",
        "        fig.show()\n",
        "        \n",
        "        return df_analysis\n",
        "\n",
        "# Apply dimensionality reduction and create visualizations\n",
        "print(\"üé® ADVANCED CLUSTER VISUALIZATION\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "visualizer = AdvancedClusterVisualizer(X_scaled, X_features, results, df_engineered)\n",
        "reduction_data = visualizer.apply_dimensionality_reduction()\n",
        "\n",
        "# Create cluster comparison plots\n",
        "visualizer.create_cluster_comparison_plot(reduction_data)\n",
        "\n",
        "# Create business insights dashboard (using best performing algorithm)\n",
        "df_with_clusters = visualizer.create_business_insights_dashboard('kmeans')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Customer Segment Profiling & Business Insights\n",
        "Detailed analysis of customer segments with actionable business recommendations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Advanced Customer Segment Profiling\n",
        "class CustomerSegmentProfiler:\n",
        "    \"\"\"\n",
        "    Advanced customer segment analysis and business insights generator\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, df_with_clusters, best_algorithm='kmeans'):\n",
        "        self.df = df_with_clusters\n",
        "        self.best_algorithm = best_algorithm\n",
        "        \n",
        "    def create_segment_profiles(self):\n",
        "        \"\"\"Create detailed profiles for each customer segment\"\"\"\n",
        "        \n",
        "        print(\"üë• CUSTOMER SEGMENT PROFILING\")\n",
        "        print(\"=\"*50)\n",
        "        \n",
        "        segments = sorted(self.df['Cluster'].unique())\n",
        "        segment_profiles = {}\n",
        "        \n",
        "        for cluster in segments:\n",
        "            if cluster == -1:  # Skip noise\n",
        "                continue\n",
        "                \n",
        "            cluster_data = self.df[self.df['Cluster'] == cluster]\n",
        "            cluster_size = len(cluster_data)\n",
        "            \n",
        "            print(f\"\\\\nüéØ CLUSTER {cluster} PROFILE\")\n",
        "            print(\"-\" * 30)\n",
        "            print(f\"üìä Size: {cluster_size} customers ({cluster_size/len(self.df)*100:.1f}%)\")\n",
        "            \n",
        "            # Demographic Profile\n",
        "            avg_age = cluster_data['Age'].mean()\n",
        "            gender_dist = cluster_data['Gender'].value_counts(normalize=True)\n",
        "            \n",
        "            print(f\"\\\\nüë§ Demographics:\")\n",
        "            print(f\"  ‚Ä¢ Average Age: {avg_age:.1f} years\")\n",
        "            print(f\"  ‚Ä¢ Gender: {gender_dist.to_dict()}\")\n",
        "            \n",
        "            # Purchase Behavior\n",
        "            avg_purchase = cluster_data['Purchase Amount (USD)'].mean()\n",
        "            avg_frequency = cluster_data['Previous Purchases'].mean()\n",
        "            \n",
        "            print(f\"\\\\nüí∞ Purchase Behavior:\")\n",
        "            print(f\"  ‚Ä¢ Average Purchase: ${avg_purchase:.2f}\")\n",
        "            print(f\"  ‚Ä¢ Average Previous Purchases: {avg_frequency:.1f}\")\n",
        "            \n",
        "            # Preferences\n",
        "            top_categories = cluster_data['Category'].value_counts().head(3)\n",
        "            top_seasons = cluster_data['Season'].value_counts().head(2)\n",
        "            freq_mode = cluster_data['Frequency of Purchases'].mode()[0]\n",
        "            \n",
        "            print(f\"\\\\nüõçÔ∏è Preferences:\")\n",
        "            print(f\"  ‚Ä¢ Top Categories: {list(top_categories.index)}\")\n",
        "            print(f\"  ‚Ä¢ Preferred Seasons: {list(top_seasons.index)}\")\n",
        "            print(f\"  ‚Ä¢ Purchase Frequency: {freq_mode}\")\n",
        "            \n",
        "            # Behavioral Indicators\n",
        "            subscription_rate = (cluster_data['Subscription Status'] == 'Yes').mean()\n",
        "            discount_usage = (cluster_data['Discount Applied'] == 'Yes').mean()\n",
        "            \n",
        "            print(f\"\\\\nüìà Behavioral Indicators:\")\n",
        "            print(f\"  ‚Ä¢ Subscription Rate: {subscription_rate:.1%}\")\n",
        "            print(f\"  ‚Ä¢ Discount Usage: {discount_usage:.1%}\")\n",
        "            \n",
        "            # Store profile data\n",
        "            segment_profiles[cluster] = {\n",
        "                'size': cluster_size,\n",
        "                'size_percentage': cluster_size/len(self.df)*100,\n",
        "                'avg_age': avg_age,\n",
        "                'gender_distribution': gender_dist.to_dict(),\n",
        "                'avg_purchase_amount': avg_purchase,\n",
        "                'avg_previous_purchases': avg_frequency,\n",
        "                'top_categories': list(top_categories.index),\n",
        "                'top_seasons': list(top_seasons.index),\n",
        "                'purchase_frequency': freq_mode,\n",
        "                'subscription_rate': subscription_rate,\n",
        "                'discount_usage': discount_usage\n",
        "            }\n",
        "        \n",
        "        return segment_profiles\n",
        "    \n",
        "    def generate_business_recommendations(self, segment_profiles):\n",
        "        \"\"\"Generate actionable business recommendations for each segment\"\"\"\n",
        "        \n",
        "        print(\"\\\\n\\\\nüí° BUSINESS RECOMMENDATIONS\")\n",
        "        print(\"=\"*50)\n",
        "        \n",
        "        # Define segment archetypes based on behavior\n",
        "        recommendations = {}\n",
        "        \n",
        "        for cluster, profile in segment_profiles.items():\n",
        "            print(f\"\\\\nüéØ CLUSTER {cluster} - MARKETING STRATEGY\")\n",
        "            print(\"-\" * 40)\n",
        "            \n",
        "            # Determine segment archetype\n",
        "            if profile['avg_purchase_amount'] > 70 and profile['subscription_rate'] > 0.8:\n",
        "                archetype = \"Premium Loyal Customers\"\n",
        "                strategy = [\n",
        "                    \"üåü VIP loyalty program with exclusive benefits\",\n",
        "                    \"üìß Personalized premium product recommendations\",\n",
        "                    \"üéÅ Early access to new collections\",\n",
        "                    \"üíé Premium customer service channel\"\n",
        "                ]\n",
        "            elif profile['discount_usage'] > 0.8 and profile['avg_purchase_amount'] < 50:\n",
        "                archetype = \"Price-Sensitive Shoppers\"\n",
        "                strategy = [\n",
        "                    \"üí∞ Targeted discount campaigns\",\n",
        "                    \"üîî Price drop notifications\",\n",
        "                    \"üì¶ Bundle deals and bulk discounts\",\n",
        "                    \"‚è∞ Flash sale notifications\"\n",
        "                ]\n",
        "            elif profile['avg_age'] < 30 and 'Clothing' in profile['top_categories']:\n",
        "                archetype = \"Young Fashion Enthusiasts\"\n",
        "                strategy = [\n",
        "                    \"üì± Social media marketing campaigns\",\n",
        "                    \"üëó Trendy and seasonal collections\",\n",
        "                    \"ü§ù Influencer partnerships\",\n",
        "                    \"üéØ Mobile-first shopping experience\"\n",
        "                ]\n",
        "            elif profile['avg_purchase_amount'] > 60 and profile['subscription_rate'] > 0.6:\n",
        "                archetype = \"Regular Value Customers\"\n",
        "                strategy = [\n",
        "                    \"üîÑ Subscription optimization programs\",\n",
        "                    \"üìà Upselling complementary products\",\n",
        "                    \"üé™ Seasonal campaigns aligned with preferences\",\n",
        "                    \"üí≥ Flexible payment options\"\n",
        "                ]\n",
        "            else:\n",
        "                archetype = \"Occasional Shoppers\"\n",
        "                strategy = [\n",
        "                    \"üì¨ Re-engagement email campaigns\",\n",
        "                    \"üéÅ Welcome back offers\",\n",
        "                    \"üìä Preference-based recommendations\",\n",
        "                    \"üîî Gentle reminder notifications\"\n",
        "                ]\n",
        "            \n",
        "            print(f\"üìã Segment Archetype: {archetype}\")\n",
        "            print(f\"üìä Size: {profile['size']} customers ({profile['size_percentage']:.1f}%)\")\n",
        "            print(f\"üí° Recommended Strategies:\")\n",
        "            for strategy_item in strategy:\n",
        "                print(f\"    {strategy_item}\")\n",
        "            \n",
        "            # Calculate potential ROI\n",
        "            current_value = profile['avg_purchase_amount'] * profile['size']\n",
        "            if archetype == \"Premium Loyal Customers\":\n",
        "                potential_uplift = 0.15  # 15% uplift\n",
        "            elif archetype == \"Price-Sensitive Shoppers\":\n",
        "                potential_uplift = 0.25  # 25% uplift through volume\n",
        "            else:\n",
        "                potential_uplift = 0.10  # 10% general uplift\n",
        "            \n",
        "            potential_value = current_value * (1 + potential_uplift)\n",
        "            roi_estimate = potential_value - current_value\n",
        "            \n",
        "            print(f\"üí∞ Current Segment Value: ${current_value:,.0f}\")\n",
        "            print(f\"üöÄ Potential Value: ${potential_value:,.0f}\")\n",
        "            print(f\"üìà Estimated ROI: ${roi_estimate:,.0f} ({potential_uplift:.0%} uplift)\")\n",
        "            \n",
        "            recommendations[cluster] = {\n",
        "                'archetype': archetype,\n",
        "                'strategies': strategy,\n",
        "                'current_value': current_value,\n",
        "                'potential_value': potential_value,\n",
        "                'roi_estimate': roi_estimate\n",
        "            }\n",
        "        \n",
        "        return recommendations\n",
        "    \n",
        "    def create_executive_summary(self, segment_profiles, recommendations):\n",
        "        \"\"\"Create executive summary of segmentation analysis\"\"\"\n",
        "        \n",
        "        print(\"\\\\n\\\\nüìä EXECUTIVE SUMMARY\")\n",
        "        print(\"=\"*60)\n",
        "        \n",
        "        total_customers = len(self.df)\n",
        "        total_segments = len(segment_profiles)\n",
        "        \n",
        "        print(f\"üéØ Customer Base: {total_customers:,} customers segmented into {total_segments} distinct groups\")\n",
        "        print(f\"üî¨ Algorithm Used: {self.best_algorithm.upper()}\")\n",
        "        \n",
        "        # Key insights\n",
        "        largest_segment = max(segment_profiles.items(), key=lambda x: x[1]['size'])\n",
        "        highest_value_segment = max(segment_profiles.items(), key=lambda x: x[1]['avg_purchase_amount'])\n",
        "        \n",
        "        print(f\"\\\\nüìà Key Insights:\")\n",
        "        print(f\"  ‚Ä¢ Largest Segment: Cluster {largest_segment[0]} ({largest_segment[1]['size_percentage']:.1f}% of customers)\")\n",
        "        print(f\"  ‚Ä¢ Highest Value Segment: Cluster {highest_value_segment[0]} (${highest_value_segment[1]['avg_purchase_amount']:.2f} avg purchase)\")\n",
        "        \n",
        "        # Calculate total ROI potential\n",
        "        total_current_value = sum([rec['current_value'] for rec in recommendations.values()])\n",
        "        total_potential_value = sum([rec['potential_value'] for rec in recommendations.values()])\n",
        "        total_roi = total_potential_value - total_current_value\n",
        "        \n",
        "        print(f\"\\\\nüí∞ Business Impact:\")\n",
        "        print(f\"  ‚Ä¢ Current Customer Value: ${total_current_value:,.0f}\")\n",
        "        print(f\"  ‚Ä¢ Potential Customer Value: ${total_potential_value:,.0f}\")\n",
        "        print(f\"  ‚Ä¢ Total ROI Opportunity: ${total_roi:,.0f}\")\n",
        "        print(f\"  ‚Ä¢ Overall Uplift Potential: {(total_roi/total_current_value)*100:.1f}%\")\n",
        "        \n",
        "        print(f\"\\\\nüöÄ Next Steps:\")\n",
        "        print(f\"  1. Implement targeted marketing campaigns for each segment\")\n",
        "        print(f\"  2. Develop segment-specific product recommendations\")\n",
        "        print(f\"  3. Create personalized customer journey maps\")\n",
        "        print(f\"  4. Monitor segment performance and adjust strategies\")\n",
        "        print(f\"  5. Regular re-segmentation to track customer evolution\")\n",
        "\n",
        "# Generate comprehensive customer insights\n",
        "profiler = CustomerSegmentProfiler(df_with_clusters)\n",
        "segment_profiles = profiler.create_segment_profiles()\n",
        "recommendations = profiler.generate_business_recommendations(segment_profiles)\n",
        "profiler.create_executive_summary(segment_profiles, recommendations)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
